{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5f66c8",
   "metadata": {
    "cellId": "teji28xy4eg56e0xcs9y1v"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import polars as pl\n",
    "import pickle\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import time\n",
    "import pyarrow.parquet as pq\n",
    "import scipy\n",
    "import implicit\n",
    "import bisect\n",
    "\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import keras_nlp\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "print(f'numpy={np.__version__}')\n",
    "print(f'pandas={pd.__version__}')\n",
    "print(f'polars={pl.__version__}')\n",
    "print(f'tf={tf.__version__}')\n",
    "print(f'tfa={tfa.__version__}')\n",
    "print(f'keras_nlp={keras_nlp.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1bdb3f",
   "metadata": {
    "cellId": "fe2fa9u29ho57aqh2cq2hr"
   },
   "outputs": [],
   "source": [
    "# Расположение папок с исходными данными\n",
    "CONFIG_ORIG_DATA_PATH = 'data/competition_data_final_pqt'\n",
    "CONFIG_ORIG_TARGET_PATH = 'data/public_train.pqt'\n",
    "CONFIG_ORIG_SUBMISSION_PATH = 'data/submit_2.pqt'\n",
    "\n",
    "# Расположение рабочих папок\n",
    "CONFIG_DATA_ENCODED_LIGHT_PARQUET_PATH = 'data_encoded_light_parquet'\n",
    "CONFIG_EMBEDDINGS_PATH = 'embeddings'\n",
    "\n",
    "CONFIG_MODEL_CHECKPOINT_ROOT = 'models'\n",
    "CONFIG_MODEL_REPORT_ROOT = 'reports'\n",
    "CONFIG_PREDICTIONS_PATH = 'predictions'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d79b463",
   "metadata": {
    "cellId": "sbmm0k8t20qmv1181ncj3"
   },
   "outputs": [],
   "source": [
    "user_id_count = 415317\n",
    "# url_host_id_count = 199683\n",
    "# cpe_model_name_id_count = 599"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5cef5d",
   "metadata": {
    "cellId": "a13l450zzc835ue9n0yorf"
   },
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "import time\n",
    "\n",
    "def logger(function):\n",
    "    @wraps(function)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        \"\"\"wrapper documentation\"\"\"\n",
    "        start = time.perf_counter()\n",
    "        print(f'{function.__name__}:begin:')        \n",
    "        output = function(*args, **kwargs)\n",
    "        end = time.perf_counter()        \n",
    "        print(f'{function.__name__}:end: took {end - start:.6f} seconds to complete')\n",
    "        return output\n",
    "    return wrapper\n",
    "\n",
    "@logger\n",
    "def add_two_numbers(a, b):\n",
    "    \"\"\"this function adds two numbers\"\"\"\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b67b1e7",
   "metadata": {
    "cellId": "2hxgm0l6rn2xdw9aqbo0ek"
   },
   "outputs": [],
   "source": [
    "@logger\n",
    "def save_embedding(embedding, name, size):\n",
    "    print(f'save_embedding: {embedding.shape} {name=} {size=}')\n",
    "    os.makedirs(CONFIG_EMBEDDINGS_PATH, exist_ok = True)    \n",
    "    file_path = Path(CONFIG_EMBEDDINGS_PATH) / f'embedding_{name}_{size:03n}.pickle'\n",
    "    print(f'save_embedding: {file_path=}')\n",
    "    \n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(embedding, f)\n",
    "\n",
    "@logger        \n",
    "def load_embedding(name, size):\n",
    "    print(f'load_embedding: {name=} {size=}')\n",
    "    file_path = Path(CONFIG_EMBEDDINGS_PATH) / f'embedding_{name}_{size:03n}.pickle'\n",
    "    print(f'load_embedding: {file_path=}')\n",
    "    \n",
    "    with open(file_path, 'rb') as f:\n",
    "        embedding = pickle.load(f)\n",
    "        \n",
    "    print(f'load_embedding: {embedding.shape}')\n",
    "    return embedding\n",
    "\n",
    "test_embedding = np.zeros((64,64), dtype=np.float32)\n",
    "save_embedding(test_embedding,'test',64)\n",
    "test_embedding2 = load_embedding('test',64)\n",
    "\n",
    "assert np.sum(test_embedding != test_embedding2) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245a8746",
   "metadata": {
    "cellId": "s65ald32o8odkia31iwq",
    "execution_id": "0d8d8117-aaec-47b1-91d9-7d3f655e41a0"
   },
   "source": [
    "# Загрузка target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e7cea",
   "metadata": {
    "cellId": "zjzyggp296qbulc5trnhdh"
   },
   "outputs": [],
   "source": [
    "@logger\n",
    "def get_targets():\n",
    "    targets = pl.read_parquet(CONFIG_ORIG_TARGET_PATH, columns=['age', 'is_male', 'user_id'])\n",
    "    print(targets.shape)\n",
    "    print(targets.head())\n",
    "\n",
    "    male_target = targets.filter((pl.col('is_male').is_in(['0', '1']))).select([pl.col('user_id').cast(pl.Int32()), pl.col('is_male')=='1'])\n",
    "    print(male_target.shape)\n",
    "    print(male_target.head())\n",
    "    print(male_target['is_male'].value_counts())\n",
    "\n",
    "    import bisect\n",
    "    age_target = targets.filter(~pl.col('age').is_null()).filter(pl.col('age') >=19).select([pl.col('user_id').cast(pl.Int32()), pl.col('age').cast(pl.Int32())])\n",
    "    age_target = age_target.with_columns(pl.col(\"age\").apply(lambda x: bisect.bisect_left([25,35,45,55,65], x)).alias(\"age_bins\")).with_columns((pl.col('age_bins')+1).alias('age_bins_pred'))\n",
    "    print(age_target.groupby(['age_bins', 'age_bins_pred']).agg([pl.col('age').min().alias('min'), pl.col('age').max().alias('max'), pl.col('age').count().alias('count')]).sort('age_bins'))\n",
    "\n",
    "    targets = male_target.join(age_target, on='user_id', how='inner')\n",
    "    print(targets.shape)\n",
    "    print(targets.head())\n",
    "    print(targets['is_male'].value_counts())\n",
    "    print(targets.groupby(['age_bins', 'age_bins_pred']).agg([pl.col('age').min().alias('min'), pl.col('age').max().alias('max'), pl.col('age').count().alias('count')]).sort('age_bins'))\n",
    "    \n",
    "    return targets\n",
    "\n",
    "# targets = get_targets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f06213c",
   "metadata": {
    "cellId": "xsi730gs4yeos5dvd1xzq"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "class cv_folds_class:\n",
    "    def __init__(self, targets, cv_fold_count=5):\n",
    "        self.targets = targets\n",
    "        self.folds = self.get_folds(targets, cv_fold_count)\n",
    "        \n",
    "    @logger\n",
    "    def get_folds(self, targets, cv_fold_count):\n",
    "        folds_path = f'cv_folds_{cv_fold_count}.parquet'\n",
    "        if not(Path(folds_path).exists()):\n",
    "            user_id = targets['user_id'].to_numpy()\n",
    "            folds = []\n",
    "            kfold = KFold(n_splits=cv_fold_count, random_state=10, shuffle=True)        \n",
    "            for fold_id, (train_index, test_index) in enumerate(kfold.split(user_id)):\n",
    "                user_id_fold = user_id[test_index]\n",
    "                fold = pl.DataFrame({'user_id':user_id_fold, 'fold_id':fold_id})\n",
    "                folds.append(fold)\n",
    "            folds = pl.concat(folds)\n",
    "            folds.write_parquet(folds_path)\n",
    "        return pl.read_parquet(folds_path)\n",
    "\n",
    "    @logger\n",
    "    def get_train_target_fold(self, fold_id):\n",
    "        print(f'get_train_target_fold: {fold_id=}')        \n",
    "        train_target_fold = self.targets.join(self.folds, on='user_id', how='inner').filter(pl.col('fold_id')!=fold_id).sample(frac=1, shuffle=True, seed=10)\n",
    "        print(f'get_train_target_fold: {train_target_fold.shape=}')        \n",
    "        return train_target_fold\n",
    "     \n",
    "    @logger\n",
    "    def get_valid_target_fold(self, fold_id):\n",
    "        print(f'get_valid_target_fold: {fold_id=}')                \n",
    "        valid_target_fold = self.targets.join(self.folds, on='user_id', how='inner').filter(pl.col('fold_id')==fold_id).sample(frac=1, shuffle=True, seed=10)\n",
    "        print(f'get_valid_target_fold: {valid_target_fold.shape=}')        \n",
    "        return valid_target_fold \n",
    "    \n",
    "    @logger\n",
    "    def get_full_target(self):\n",
    "        target = self.targets.sample(frac=1, shuffle=True, seed=10)\n",
    "        print(f'get_full_target: {target.shape=}')        \n",
    "        return target    \n",
    "    \n",
    "    @logger\n",
    "    def get_test_target(self):\n",
    "        user_pl = pl.read_parquet(CONFIG_ORIG_SUBMISSION_PATH, columns=['user_id'])\n",
    "        user_pl = user_pl.select([\n",
    "            pl.col('user_id').cast(pl.Int32()),\n",
    "            pl.lit(-1).alias('is_male'),\n",
    "            pl.lit(-1).alias('age'),\n",
    "            pl.lit(-1).alias('age_bins')        \n",
    "        ])\n",
    "        print(user_pl)\n",
    "        return user_pl\n",
    "    \n",
    "cv_folds = cv_folds_class(targets=get_targets(), cv_fold_count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26a9c73",
   "metadata": {
    "cellId": "or3ktuqs1rrzfgfmpnyq0e",
    "execution_id": "17522f02-35c7-4143-b487-67861e30780c"
   },
   "source": [
    "# Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c06d15c",
   "metadata": {
    "cellId": "inki3iivgufhlrhe7wqj3v"
   },
   "outputs": [],
   "source": [
    "@logger\n",
    "def get_url_host_id_map():\n",
    "    url_dict = pl.read_parquet('/home/jupyter/mnt/s3/mtsmlcup/dicts/category_dict_url_host.parquet') #, columns=['url_host_id', 'url_host']\n",
    "    url_dict = url_dict.with_columns(pl.col('url_host').apply(lambda x: x.encode('idna').decode('idna')).alias('url_host'))\n",
    "    print(url_dict.shape)\n",
    "\n",
    "    # Новая колонка для очищенных URL\n",
    "    url_dict = url_dict.with_columns(pl.col('url_host').alias('url_host_clean'))\n",
    "\n",
    "    # Заменяем цифры на N (только в URL)\n",
    "    url_dict = url_dict.with_columns(pl.when(pl.col('url_host_clean').str.contains(r'^.*\\.[a-z]*$')).then(pl.col('url_host_clean').str.replace_all(r'\\d+', 'N')).otherwise(pl.col('url_host_clean')).alias('url_host_clean'))\n",
    "\n",
    "    # Заменяем URL, который был меньше чем у N пользователей на 'lessthanNusers'\n",
    "#     N = 2\n",
    "#     url_dict = url_dict.with_columns(pl.when(pl.col('user_id_count') < N).then(pl.lit('lessthanNusers')).otherwise(pl.col('url_host_clean')).alias('url_host_clean'))\n",
    "#     print(url_dict)\n",
    "    \n",
    "    # Удаляем из истории записи с lessthanNusers\n",
    "#     url_dict = url_dict.filter(pl.col('url_host_clean') != 'lessthanNusers')\n",
    "#     print(url_dict)\n",
    "    \n",
    "    # url_dict = url_dict.groupby('url_host_clean').agg([pl.all(), pl.count().alias('url_host_count')]).sort('url_host_count', descending=True)\n",
    "    # print(url_dict)\n",
    "\n",
    "    url_dict = url_dict.groupby('url_host_clean').agg([pl.col('url_host_id')]).with_columns(pl.arange(low=0, high=pl.count()).cast(pl.Int32()).alias('url_host_clean_id'))\n",
    "    url_dict = url_dict.select(['url_host_id', 'url_host_clean_id'])\n",
    "    print(url_dict.shape)\n",
    "    print(f\"url_host_clean_id min={url_dict['url_host_clean_id'].min()} min={url_dict['url_host_clean_id'].max()} n_unique={url_dict['url_host_clean_id'].n_unique()}\")\n",
    "\n",
    "    url_dict = url_dict.explode('url_host_id')\n",
    "    print(url_dict.shape)\n",
    "    \n",
    "    return url_dict\n",
    "\n",
    "get_url_host_id_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae30bea",
   "metadata": {
    "cellId": "m2ppdubr4qo71rq33anhoa"
   },
   "outputs": [],
   "source": [
    "@logger\n",
    "def get_url_stat_by_user(n_files=100):\n",
    "    \"\"\"\n",
    "    История посещения пользователем url. \n",
    "    Количество посещений URL считается суммой за все даты, поделенной на количество дат, за которые у пользователя были посещения\n",
    "    \"\"\"\n",
    "    url_host_id_map = get_url_host_id_map()\n",
    "        \n",
    "    data = []\n",
    "    for i, file_path in enumerate(list(Path(CONFIG_DATA_ENCODED_LIGHT_PARQUET_PATH).glob('*.parquet'))[:n_files]):\n",
    "        print(f'reading {file_path}')\n",
    "        data_item = pl.scan_parquet(file_path)\n",
    "        print(data_item.columns)\n",
    "        \n",
    "        # Вычищаем часть URL через url_host_id_map\n",
    "        data_item = data_item.join(url_host_id_map.lazy(), on='url_host_id').select(pl.exclude('url_host_id')).rename({'url_host_clean_id':'url_host_id'})        \n",
    "        \n",
    "        # Считаем посещения за все дни и количество дней с посещениями\n",
    "        part_of_day_id_2code_mapper = {\n",
    "            0: 1,\n",
    "            1: 2,\n",
    "            2: 4,\n",
    "            3: 8\n",
    "        }\n",
    "\n",
    "        data_item = data_item.with_columns(\n",
    "            pl.col('part_of_day_id').map_dict(part_of_day_id_2code_mapper, default=pl.col(\"part_of_day_id\")).alias('part_of_day_code')\n",
    "        )\n",
    "\n",
    "        data_item_url_stat = data_item.groupby(['user_id', 'url_host_id']).agg([\n",
    "            pl.col('request_cnt').sum().alias('request_cnt_total'),\n",
    "            pl.col('part_of_day_id').mode().max().alias('part_of_day_id_mode'),\n",
    "#             pl.col('part_of_day_code').unique().str.concat(',').alias('part_of_day_code_list'),\n",
    "            pl.col('part_of_day_code').unique().sum().alias('part_of_day_code'),            \n",
    "            pl.col('date_int').n_unique().alias('n_days_url')\n",
    "        ])\n",
    "                \n",
    "        # Считаем количество дней у пользовальтелей\n",
    "        data_item_date_stat = data_item.groupby('user_id').agg(pl.col('date_int').n_unique().alias('n_days_any_url'))\n",
    "        \n",
    "        # Считаем статистику посещения в день\n",
    "        data_item = data_item_url_stat.join(data_item_date_stat, on='user_id')\n",
    "        \n",
    "#         # Группируем в одну строчку на пользователя\n",
    "#         data_item = data_item.sort(['user_id', 'request_cnt'], descending=[False, True]).groupby(['user_id']).agg([\n",
    "#             pl.col('url_host_id'),\n",
    "#             pl.count().alias('url_host_id_count'),              \n",
    "#             pl.col('request_cnt'),\n",
    "#         ])\n",
    "        data.append(data_item)\n",
    "        del data_item\n",
    "    data = pl.collect_all(data)\n",
    "    data = pl.concat(data)\n",
    "    return data\n",
    "\n",
    "# get_url_stat_by_user(n_files=1).groupby(['part_of_day_code_list', 'part_of_day_code']).agg(pl.count()).sort('part_of_day_code')\n",
    "url_stat_by_user = get_url_stat_by_user(n_files=100) \n",
    "print(url_stat_by_user.groupby('user_id').agg([pl.count()]).select([\n",
    "    pl.col('count').min().alias('min'),\n",
    "    pl.col('count').mean().alias('mean'),\n",
    "   pl.col('count').max().alias('max')\n",
    "])) # max user history len = 1621"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0617bb8d",
   "metadata": {
    "cellId": "slu6y9wa6rujb3lrzqnd"
   },
   "outputs": [],
   "source": [
    "# ┌─────┬───────────┬──────┐\n",
    "# │ min ┆ mean      ┆ max  │\n",
    "# │ --- ┆ ---       ┆ ---  │\n",
    "# │ u32 ┆ f64       ┆ u32  │\n",
    "# ╞═════╪═══════════╪══════╡\n",
    "# │ 1   ┆ 72.030008 ┆ 1621 │\n",
    "# └─────┴───────────┴──────┘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fb7614",
   "metadata": {
    "cellId": "0ob5f3sm7f6mrjtk2jryg"
   },
   "outputs": [],
   "source": [
    "@logger\n",
    "def get_category_feature(column):\n",
    "    print(f'get_category_feature:begin {column=}')    \n",
    "    \n",
    "    data_list = []\n",
    "    for i, file_path in enumerate(list(Path(CONFIG_DATA_ENCODED_LIGHT_PARQUET_PATH).glob('*.parquet'))[:]):\n",
    "        print(f'reading {file_path}')\n",
    "        data = pl.scan_parquet(file_path)\n",
    "        \n",
    "        data = data.groupby(['user_id', column]).agg(pl.col('date_int').n_unique().alias('days')).sort(['user_id', 'days', column], descending=[False, True, False])\n",
    "        data = data.groupby('user_id').agg([\n",
    "            pl.col(column).first().alias(f'{column}_primary'),\n",
    "            pl.col(column).len().alias(f'{column}_count'),\n",
    "        ]).sort('user_id')        \n",
    "        data_list.append(data)\n",
    "    data_list = pl.collect_all(data_list)\n",
    "    data = pl.concat(data_list)\n",
    "    return data\n",
    "\n",
    "@logger\n",
    "def get_category_features():\n",
    "    data_list = []\n",
    "    for column in ['region_name_id', 'city_name_id', 'cpe_manufacturer_name_id', 'cpe_model_name_id', 'cpe_type_cd_id', 'cpe_model_os_type_id']: # 'part_of_day'\n",
    "        print(f'{column=}')\n",
    "        data_list.append(get_category_feature(column))\n",
    "    data_list = pl.concat([data_list[0].select([pl.col('user_id')])]+[i.select(pl.exclude('user_id')) for i in data_list], how='horizontal')\n",
    "    return data_list\n",
    "\n",
    "cat_features = get_category_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42acbe92",
   "metadata": {
    "cellId": "521gfquolw55qu8oc17e1q"
   },
   "outputs": [],
   "source": [
    "@logger\n",
    "def get_price_features():\n",
    "    data_list = []\n",
    "    for i, file_path in enumerate(list(Path(CONFIG_DATA_ENCODED_LIGHT_PARQUET_PATH).glob('*.parquet'))[:]):\n",
    "        print(f'reading {file_path}')\n",
    "        data = pl.scan_parquet(file_path)\n",
    "        \n",
    "        data = data.groupby(['user_id']).agg(pl.col('price').max())\n",
    "        data = data.sort('user_id')        \n",
    "        data_list.append(data)\n",
    "    data_list = pl.collect_all(data_list)\n",
    "    data = pl.concat(data_list)\n",
    "    print(f'get_category_feature:end')            \n",
    "    return data\n",
    "\n",
    "price_features = get_price_features()\n",
    "print(price_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e6b20d",
   "metadata": {
    "cellId": "mly7m0yn1gqpprn5uae09r"
   },
   "outputs": [],
   "source": [
    "@logger\n",
    "def get_normalization_layer(data, name):\n",
    "    print(f'get_normalization_layer: {name=}')\n",
    "    \n",
    "    mean = np.mean(data)\n",
    "    variance = np.var(data)\n",
    "    print(f'get_normalization_layer: {mean=} {variance=}')\n",
    "    \n",
    "    layer = tf.keras.layers.Normalization(axis=None, mean=mean, variance=variance, name=name)\n",
    "    return layer\n",
    "\n",
    "get_normalization_layer(np.arange(100), 'temp')(tf.constant([0,1,2,3,4,50,99]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d539e097",
   "metadata": {
    "cellId": "y47i5klf1rb9x9a2ds5ht"
   },
   "outputs": [],
   "source": [
    "@logger\n",
    "def get_discretization_layer(data, n_bins, name):\n",
    "    print(f'get_discretization_layer: {n_bins=} {name=}')\n",
    "    \n",
    "    quantiles = np.linspace(0, 1, n_bins + 1)\n",
    "    print(quantiles)\n",
    "\n",
    "    bins = np.quantile(data, quantiles)\n",
    "    print(f'get_discretization_layer: {bins=} {bins.shape=}')\n",
    "    \n",
    "    bins = np.unique(bins)\n",
    "    print(f'get_discretization_layer: {bins=} {bins.shape=}')\n",
    "    \n",
    "    data_bins = np.digitize(data, bins, right=True)\n",
    "    stat = np.unique(data_bins, return_counts=True)\n",
    "    print(f'get_discretization_layer: bins_value: {stat[0]}')\n",
    "    print(f'get_discretization_layer: bins_count: {stat[1]}')\n",
    "    \n",
    "    layer = tf.keras.layers.Discretization(bin_boundaries=bins[1:-1], epsilon=0.01, name=name)\n",
    "    return layer\n",
    "\n",
    "get_discretization_layer(data=url_stat_by_user['request_cnt_total'].to_numpy() / url_stat_by_user['n_days_url'].to_numpy(), n_bins=100, name='temp1')(np.arange(1000))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b06c90",
   "metadata": {
    "cellId": "c7v6woq574d98ud9etddzk",
    "execution_id": "81a7df07-d7a2-4b7b-a5d3-12075685503d"
   },
   "source": [
    "# Формируем словарь для перекодировки url_host_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2947217c",
   "metadata": {
    "cellId": "1d4cblfyyyu9s609dael9s"
   },
   "outputs": [],
   "source": [
    "class enconding_config_class:\n",
    "    @logger\n",
    "    def __init__(self):\n",
    "        self.vocabulary = enconding_config_class.get_encoding_vocabulary()\n",
    "        print(f'enconding_config_class: {self.vocabulary.shape=}')\n",
    "        \n",
    "        self.pad_token = -1\n",
    "        self.pad_token_id = 0\n",
    "        print(f'enconding_config_class: {self.pad_token=} {self.pad_token_id=}')           \n",
    "        \n",
    "        self.oov_token_id = 1\n",
    "        print(f'enconding_config_class: {self.oov_token_id=}') \n",
    "        \n",
    "        self.mask_token = -2\n",
    "        self.mask_token_id = 2\n",
    "        print(f'enconding_config_class: {self.mask_token=} {self.mask_token_id=}')  \n",
    "        \n",
    "        self.start_token = -3\n",
    "        self.start_token_id = 3\n",
    "        print(f'enconding_config_class: {self.start_token=} {self.start_token_id=}')   \n",
    "        \n",
    "        self.end_token = -4\n",
    "        self.end_token_id = 4\n",
    "        print(f'enconding_config_class: {self.end_token=} {self.end_token_id=}')           \n",
    "        \n",
    "        self.vocabulary = np.append([self.mask_token, self.start_token, self.end_token], self.vocabulary)\n",
    "        print(f'enconding_config_class: adding mask token: {self.vocabulary.shape=}')        \n",
    "        \n",
    "        self.vocabulary_size_short = len(self.vocabulary)\n",
    "        print(f'enconding_config_class: {self.vocabulary_size_short=}')            \n",
    "        \n",
    "        self.vocabulary_size_full = self.vocabulary_size_short + 2 # + mask + oov\n",
    "        print(f'enconding_config_class: {self.vocabulary_size_full=}')                  \n",
    "        \n",
    "    @logger        \n",
    "    def get_encoding_vocabulary():\n",
    "        url_host_id_all = url_stat_by_user['url_host_id'].to_numpy()\n",
    "        values, values_count = np.unique(url_host_id_all, return_counts=True)\n",
    "        vocabulary = values[values_count >= 2]\n",
    "        vocabulary = np.sort(vocabulary)\n",
    "        np.savetxt('vocabulary.csv', vocabulary, delimiter=',', fmt='%d') \n",
    "#         vocabulary = values[values_count >= 10]\n",
    "        return vocabulary        \n",
    "    \n",
    "    @logger        \n",
    "    def get_tokenizer(self):\n",
    "        tokenizer = tf.keras.layers.IntegerLookup(\n",
    "            output_mode='int',\n",
    "            mask_token=self.pad_token, # will be 0\n",
    "            num_oov_indices=1, # will be 1\n",
    "            vocabulary=self.vocabulary # will start from 2 ()\n",
    "        )  \n",
    "        return tokenizer            \n",
    "    \n",
    "enconding_config = enconding_config_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853de89e",
   "metadata": {
    "cellId": "tlk8ael845o2lzulxpgvty",
    "execution_id": "f423d3c6-aa94-4dac-b13a-7f63c7bf6be8"
   },
   "source": [
    "# Создаем datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef79b97",
   "metadata": {
    "cellId": "1ybz6b9tlj6y100hv15d8g"
   },
   "outputs": [],
   "source": [
    "cv_fold_id=0\n",
    "\n",
    "@logger\n",
    "def get_data_by_target(target):\n",
    "    print('Данные из истории посещения')\n",
    "    data = url_stat_by_user\n",
    "    print(data)\n",
    "    \n",
    "    print('Группированная история посещения')\n",
    "    data = url_stat_by_user.sort('request_cnt_total', descending=True).groupby(['user_id']).agg([\n",
    "        pl.col('url_host_id'),\n",
    "        pl.col('request_cnt_total'),\n",
    "        pl.col('part_of_day_id_mode'),       \n",
    "        pl.col('part_of_day_code'),\n",
    "        pl.col('n_days_url'),\n",
    "        pl.col('n_days_any_url').first(),\n",
    "        pl.count().alias('url_host_id_count'),\n",
    "    ])\n",
    "    print(data)\n",
    "    \n",
    "    print('Добавляем cat_features')\n",
    "    data = cat_features.join(data, on='user_id')\n",
    "    print(data)\n",
    "    \n",
    "    print('Добавляем price_features')    \n",
    "    data = price_features.join(data, on='user_id')\n",
    "    print(data)\n",
    "        \n",
    "    print('Добавляем targets')\n",
    "    data = target.join(data, on='user_id')\n",
    "    print(data)\n",
    "    \n",
    "    return data\n",
    "        \n",
    "train_sample = get_data_by_target(cv_folds.get_train_target_fold(cv_fold_id))[:10]\n",
    "train_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd69cfd",
   "metadata": {
    "cellId": "wnzpbmsis9ljvano2sowj"
   },
   "outputs": [],
   "source": [
    "@logger\n",
    "def get_pretrain_train_valid_df():\n",
    "    print('Данные из истории посещения')\n",
    "    data = url_stat_by_user\n",
    "    print(data)\n",
    "    \n",
    "    print('Группированная история посещения')\n",
    "    data = url_stat_by_user.sort('request_cnt_total', descending=True).groupby(['user_id']).agg([\n",
    "        pl.col('url_host_id'),\n",
    "        pl.col('request_cnt_total'),\n",
    "        pl.col('part_of_day_id_mode'),       \n",
    "        pl.col('part_of_day_code'),\n",
    "        pl.col('n_days_url'),\n",
    "        pl.col('n_days_any_url').first(),\n",
    "        pl.count().alias('url_host_id_count'),\n",
    "    ])\n",
    "    print(data)\n",
    "    \n",
    "    print('Добавляем cat_features')\n",
    "    data = cat_features.join(data, on='user_id')\n",
    "    print(data)\n",
    "    \n",
    "    print('Добавляем price_features')    \n",
    "    data = price_features.join(data, on='user_id')\n",
    "    print(data)\n",
    "    \n",
    "    print('Перемешиваем и добавляем пустой таргет')\n",
    "    data = data.sample(frac=1, shuffle=True, seed=10).with_columns([\n",
    "        pl.lit(-1).alias('is_male'),\n",
    "        pl.lit(-1).alias('age'),\n",
    "        pl.lit(-1).alias('age_bins') \n",
    "    ])\n",
    "    print(data)\n",
    "    \n",
    "    print('Выделяем train')\n",
    "    train = data[:-10000]\n",
    "    print(train)\n",
    "    \n",
    "    print('Выделяем valid')\n",
    "    valid = data[-10000:]\n",
    "    print(valid) \n",
    "    \n",
    "    return train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3449f30",
   "metadata": {
    "cellId": "j8yub3n4ntrvxhed10tlcb"
   },
   "outputs": [],
   "source": [
    "@logger\n",
    "def get_dataset(df):\n",
    "    print(f'get_dataset:begin {df.shape=}')\n",
    "    \n",
    "    @logger\n",
    "    def get_ragged_tensor(df, column_name):\n",
    "        # https://github.com/tensorflow/tensorflow/issues/47853    \n",
    "        print(f'get_dataset:get_ragged_tensor: {column_name}')        \n",
    "        ragged = tf.RaggedTensor.from_row_lengths(\n",
    "            values=np.hstack(df[column_name]),\n",
    "            row_lengths=df['url_host_id_count'],\n",
    "        )\n",
    "        return ragged\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        {\n",
    "            # User\n",
    "            'user_id':df['user_id'].to_numpy(),\n",
    "            \n",
    "            # History fields\n",
    "            'url_host_id':get_ragged_tensor(df, 'url_host_id'),\n",
    "            'request_cnt_total':get_ragged_tensor(df, 'request_cnt_total'),    \n",
    "            'part_of_day_id_mode':get_ragged_tensor(df, 'part_of_day_id_mode'),   \n",
    "            'part_of_day_code':get_ragged_tensor(df, 'part_of_day_code'),   \n",
    "            'n_days_url':get_ragged_tensor(df, 'n_days_url'),  \n",
    "            'n_days_any_url':df['n_days_any_url'].to_numpy(),  \n",
    "            'url_host_id_count':df['url_host_id_count'].to_numpy(),\n",
    "            \n",
    "#             # Static fields\n",
    "            'region_name_id_primary':df['region_name_id_primary'].to_numpy(),\n",
    "            'city_name_id_primary':df['city_name_id_primary'].to_numpy(),\n",
    "            'cpe_manufacturer_name_id_primary':df['cpe_manufacturer_name_id_primary'].to_numpy(),\n",
    "            'cpe_model_name_id_primary':df['cpe_model_name_id_primary'].to_numpy(),\n",
    "            'cpe_type_cd_id_primary':df['cpe_type_cd_id_primary'].to_numpy(),\n",
    "            'cpe_model_os_type_id_primary':df['cpe_model_os_type_id_primary'].to_numpy(),\n",
    "            'price':df['price'].to_numpy(),\n",
    "            'region_name_id_count':df['region_name_id_count'].to_numpy(),\n",
    "            'city_name_id_count':df['city_name_id_count'].to_numpy(),\n",
    "            'cpe_model_name_id_count':df['cpe_model_name_id_count'].to_numpy(),\n",
    "            'url_host_id_count':df['url_host_id_count'].to_numpy(),\n",
    "        },\n",
    "        {\n",
    "            'is_male':df['is_male'].to_numpy(),\n",
    "            'age_bins':tf.one_hot(df['age_bins'].to_numpy(), 6),\n",
    "            'age':df['age'].to_numpy(),     \n",
    "        }))\n",
    "    print(f'get_dataset:end')    \n",
    "    return dataset\n",
    "\n",
    "@logger\n",
    "def apply_encode_host_id(dataset):\n",
    "    tokenizer = enconding_config.get_tokenizer()\n",
    "    \n",
    "    def fn(inputs, target):\n",
    "        url_host_id = inputs['url_host_id']\n",
    "        url_host_id = tokenizer(url_host_id)        \n",
    "        inputs['url_host_id'] = url_host_id\n",
    "        \n",
    "        return inputs, target\n",
    "    \n",
    "    dataset = dataset.map(fn, num_parallel_calls=tf.data.AUTOTUNE) \n",
    "    return dataset\n",
    "\n",
    "def ragged_to_tensor(item, target):\n",
    "    item['url_host_id'] = item['url_host_id'].to_tensor(default_value=-1)\n",
    "    item['request_cnt_total'] = item['request_cnt_total'].to_tensor(default_value=-1)\n",
    "    item['part_of_day_id_mode'] = item['part_of_day_id_mode'].to_tensor(default_value=-1)\n",
    "    item['part_of_day_code'] = item['part_of_day_code'].to_tensor(default_value=-1)\n",
    "    item['n_days_url'] = item['n_days_url'].to_tensor(default_value=-1)\n",
    "    return item, target\n",
    "\n",
    "@logger\n",
    "def get_train_dataset(df, batch_size, shuffle_len=1024, data_rate=0.75, min_data_size=5):\n",
    "    print(f'get_train_dataset:begin {batch_size=}')\n",
    "    dataset = get_dataset(df)\n",
    "    dataset = dataset.shuffle(shuffle_len)\n",
    "    \n",
    "    def appy_random_mask(dataset, data_rate, min_data_size):\n",
    "        def random_drop(item, target):\n",
    "            item_mask = tf.random.uniform(shape=tf.shape(item['url_host_id']))\n",
    "            item_mask = item_mask <= data_rate\n",
    "            item_mask_size = tf.reduce_sum(tf.cast(item_mask, tf.int32))\n",
    "            item_mask = tf.where(item_mask_size >= min_data_size, item_mask, tf.ones_like(item_mask, dtype=tf.bool))\n",
    "            item['url_host_id'] = tf.ragged.boolean_mask(data=item['url_host_id'], mask=item_mask)\n",
    "            item['request_cnt_total'] = tf.ragged.boolean_mask(data=item['request_cnt_total'], mask=item_mask)\n",
    "            item['part_of_day_id_mode'] = tf.ragged.boolean_mask(data=item['part_of_day_id_mode'], mask=item_mask)\n",
    "            item['part_of_day_code'] = tf.ragged.boolean_mask(data=item['part_of_day_code'], mask=item_mask)\n",
    "            item['n_days_url'] = tf.ragged.boolean_mask(data=item['n_days_url'], mask=item_mask)        \n",
    "            return item, target\n",
    "    \n",
    "        dataset = dataset.map(random_drop, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "        return dataset\n",
    "    \n",
    "    dataset = appy_random_mask(dataset, data_rate=data_rate, min_data_size=min_data_size)\n",
    "    dataset = dataset.apply(tf.data.experimental.dense_to_ragged_batch(batch_size=batch_size))\n",
    "    dataset = dataset.map(ragged_to_tensor, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    dataset = apply_encode_host_id(dataset)        \n",
    "    \n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)    \n",
    "    print(f'get_train_dataset:end')    \n",
    "    return dataset\n",
    "\n",
    "@logger\n",
    "def get_valid_dataset(df, batch_size):\n",
    "    print(f'get_valid_dataset:begin {batch_size=}')\n",
    "    dataset = get_dataset(df)\n",
    "    dataset = dataset.apply(tf.data.experimental.dense_to_ragged_batch(batch_size=batch_size))\n",
    "    dataset = dataset.map(ragged_to_tensor, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    dataset = apply_encode_host_id(dataset)    \n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    print(f'get_valid_dataset:end')    \n",
    "    return dataset\n",
    "\n",
    "    \n",
    "###############################################################################\n",
    "\n",
    "# for i in get_dataset(train_sample).take(2):\n",
    "#     print(i[0]['url_host_id'])\n",
    "    \n",
    "for i in get_train_dataset(train_sample, batch_size=2).take(2):\n",
    "    print(i[0]['url_host_id'])\n",
    "    \n",
    "# for i in get_valid_dataset(train_sample, batch_size=2).take(1):\n",
    "#     print(i[0]['url_host_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57355a8",
   "metadata": {
    "cellId": "qfh6qob3189xojpzefsn9"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in tqdm(get_train_dataset(get_data_by_target(cv_folds.get_train_target_fold(cv_fold_id)), batch_size=32)):\n",
    "    i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07808631",
   "metadata": {
    "cellId": "1wtu62kohz9ta4883w3ud"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in tqdm(get_valid_dataset(get_data_by_target(cv_folds.get_valid_target_fold(cv_fold_id)), batch_size=32)):\n",
    "    i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4674b161",
   "metadata": {
    "cellId": "bgil1azci671apcdmuymyz",
    "execution_id": "ff8c55e1-0dff-4d3e-9ccf-51601c824c88"
   },
   "source": [
    "# Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e10c09",
   "metadata": {
    "cellId": "b4j43g9ibckd9zgcisg2g"
   },
   "outputs": [],
   "source": [
    "def get_model_checkpoint_path_old(model_name: str) -> str:\n",
    "    model_checkpoin_path = os.path.join(CONFIG_MODEL_CHECKPOINT_ROOT, model_name, 'checkpoint')\n",
    "    print(f'get_model_checkpoint_path: {model_checkpoin_path}')\n",
    "    return model_checkpoin_path\n",
    "\n",
    "def get_model_checkpoint_path(model_name: str) -> str:\n",
    "    os.makedirs(CONFIG_MODEL_CHECKPOINT_ROOT, exist_ok=True)    \n",
    "    model_checkpoin_path = os.path.join(CONFIG_MODEL_CHECKPOINT_ROOT, model_name, '{epoch:02d}_checkpoint')\n",
    "    print(f'get_model_checkpoint_path: {model_checkpoin_path}')\n",
    "    return model_checkpoin_path\n",
    "\n",
    "def get_model_checkpoint_path_by_epoch(model_name: str, epoch: int) -> str:\n",
    "    model_checkpoin_path = os.path.join(CONFIG_MODEL_CHECKPOINT_ROOT, model_name, f'{epoch:02d}_checkpoint')\n",
    "    print(f'get_model_checkpoint_path_by_epoch: model_name={model_name} epoch={epoch} -> {model_checkpoin_path}')\n",
    "    return model_checkpoin_path\n",
    "\n",
    "def get_model_report_path(model_name: str) -> str:\n",
    "    os.makedirs(CONFIG_MODEL_REPORT_ROOT, exist_ok=True)\n",
    "    model_report_path = os.path.join(CONFIG_MODEL_REPORT_ROOT, f'{model_name}_report.csv')\n",
    "    print(f'get_model_report_path: {model_report_path}')\n",
    "    return model_report_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5f268a",
   "metadata": {
    "cellId": "1lv7ovt3uttimebl88xmgd"
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def categorical_focal_loss(gamma=2.0, alpha=0.25):\n",
    "    \"\"\"\n",
    "    Implementation of Focal Loss from the paper in multiclass classification\n",
    "    Formula:\n",
    "        loss = -alpha*((1-p)^gamma)*log(p)\n",
    "    Parameters:\n",
    "        alpha -- the same as wighting factor in balanced cross entropy\n",
    "        gamma -- focusing parameter for modulating factor (1-p)\n",
    "    Default value:\n",
    "        gamma -- 2.0 as mentioned in the paper\n",
    "        alpha -- 0.25 as mentioned in the paper\n",
    "    \"\"\"\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        # Define epsilon so that the backpropagation will not result in NaN\n",
    "        # for 0 divisor case\n",
    "        epsilon = K.epsilon()\n",
    "        # Add the epsilon to prediction value\n",
    "        #y_pred = y_pred + epsilon\n",
    "        # Clip the prediction value\n",
    "        y_pred = K.clip(y_pred, epsilon, 1.0-epsilon)\n",
    "        # Calculate cross entropy\n",
    "        cross_entropy = -y_true*K.log(y_pred)\n",
    "        # Calculate weight that consists of  modulating factor and weighting factor\n",
    "        weight = alpha * y_true * K.pow((1-y_pred), gamma)\n",
    "        # Calculate focal loss\n",
    "        loss = weight * cross_entropy\n",
    "        # Sum the losses in mini_batch\n",
    "        loss = K.sum(loss, axis=1)\n",
    "        return loss\n",
    "    \n",
    "    return focal_loss\n",
    "\n",
    "y_true = tf.constant([[0., 1., 0.], [0., 0., 1.]])\n",
    "y_pred = tf.constant([[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n",
    "categorical_focal_loss()(y_true, y_pred).numpy().mean()\n",
    "#0.23315276"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1754e739",
   "metadata": {
    "cellId": "guyz8xwyqef01q8e0224s"
   },
   "outputs": [],
   "source": [
    "from keras import backend_config\n",
    "epsilon = backend_config.epsilon\n",
    "\n",
    "def categorical_focal_crossentropy(\n",
    "    target,\n",
    "    output,\n",
    "    alpha=0.25,\n",
    "    gamma=2.0,\n",
    "    from_logits=False,\n",
    "    axis=-1,\n",
    "):\n",
    "    \"\"\"Computes the alpha balanced focal crossentropy loss between\n",
    "    the labels and predictions.\n",
    "    According to [Lin et al., 2018](https://arxiv.org/pdf/1708.02002.pdf), it\n",
    "    helps to apply a focal factor to down-weight easy examples and focus more on\n",
    "    hard examples. By default, the focal tensor is computed as follows:\n",
    "    It has pt defined as:\n",
    "    pt = p, if y = 1 else 1 - p\n",
    "    The authors use alpha-balanced variant of focal loss in the paper:\n",
    "    FL(pt) = −α_t * (1 − pt)^gamma * log(pt)\n",
    "    Extending this to multi-class case is straightforward:\n",
    "    FL(pt) = α_t * (1 − pt)^gamma * CE, where minus comes from\n",
    "    negative log-likelihood and included in CE.\n",
    "    `modulating_factor` is (1 − pt)^gamma, where `gamma` is a focusing\n",
    "    parameter. When `gamma` = 0, there is no focal effect on the categorical\n",
    "    crossentropy. And if alpha = 1, at the same time the loss is equivalent\n",
    "    to the categorical crossentropy.\n",
    "    Args:\n",
    "        target: A tensor with the same shape as `output`.\n",
    "        output: A tensor.\n",
    "        alpha: A weight balancing factor for all classes, default is `0.25` as\n",
    "            mentioned in the reference. It can be a list of floats or a scalar.\n",
    "            In the multi-class case, alpha may be set by inverse class\n",
    "            frequency by using `compute_class_weight` from `sklearn.utils`.\n",
    "        gamma: A focusing parameter, default is `2.0` as mentioned in the\n",
    "            reference. It helps to gradually reduce the importance given to\n",
    "            simple examples in a smooth manner.\n",
    "        from_logits: Whether `output` is expected to be a logits tensor. By\n",
    "            default, we consider that `output` encodes a probability\n",
    "            distribution.\n",
    "    Returns:\n",
    "        A tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    def _constant_to_tensor(x, dtype):\n",
    "        \"\"\"Convert the input `x` to a tensor of type `dtype`.\n",
    "        This is slightly faster than the _to_tensor function, at the cost of\n",
    "        handling fewer cases.\n",
    "        Args:\n",
    "            x: An object to be converted (numpy arrays, floats, ints and lists of\n",
    "              them).\n",
    "            dtype: The destination type.\n",
    "        Returns:\n",
    "            A tensor.\n",
    "        \"\"\"\n",
    "        return tf.constant(x, dtype=dtype)\n",
    "\n",
    "\n",
    "\n",
    "    target = tf.convert_to_tensor(target)\n",
    "    output = tf.convert_to_tensor(output)\n",
    "    target.shape.assert_is_compatible_with(output.shape)\n",
    "\n",
    "    # scale preds so that the class probas of each sample sum to 1\n",
    "    output = output / tf.reduce_sum(output, axis=axis, keepdims=True)\n",
    "\n",
    "    epsilon_ = _constant_to_tensor(epsilon(), output.dtype.base_dtype)\n",
    "    output = tf.clip_by_value(output, epsilon_, 1.0 - epsilon_)\n",
    "\n",
    "    # Calculate cross entropy\n",
    "    cce = -target * tf.math.log(output)\n",
    "\n",
    "    # Calculate factors\n",
    "    modulating_factor = tf.pow(1.0 - output, gamma)\n",
    "    weighting_factor = tf.multiply(modulating_factor, alpha)\n",
    "\n",
    "    # Apply weighting factor\n",
    "    focal_cce = tf.multiply(weighting_factor, cce)\n",
    "    focal_cce = tf.reduce_sum(focal_cce, axis=axis)\n",
    "    return focal_cce\n",
    "\n",
    "tf.reduce_mean(categorical_focal_crossentropy(y_true, y_pred, alpha=0.25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5599656a",
   "metadata": {
    "cellId": "159l90q9et6vidydimkif8"
   },
   "outputs": [],
   "source": [
    "def get_categorical_focal_crossentropy_loss(gamma=2.0, alpha=0.25):\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        return categorical_focal_crossentropy(y_true, y_pred, alpha=alpha, gamma=gamma)\n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d6cbb6",
   "metadata": {
    "cellId": "0cfxnsa97f48rva0ofuwj6e"
   },
   "outputs": [],
   "source": [
    "import keras_nlp \n",
    "\n",
    "NORM_EPSILON = 1e-5\n",
    "\n",
    "@logger\n",
    "def get_encoder_inputs():\n",
    "    inputs = {}\n",
    "    inputs['url_host_id'] = tf.keras.Input(name='url_host_id', ragged=False, dtype=tf.int32, shape=(None,))   \n",
    "    inputs['request_cnt_total'] = tf.keras.Input(name='request_cnt_total', ragged=False, dtype=tf.int32, shape=(None,))\n",
    "    inputs['n_days_url'] = tf.keras.Input(name='n_days_url', ragged=False, dtype=tf.int32, shape=(None,))\n",
    "    inputs['part_of_day_id_mode'] = tf.keras.Input(name='part_of_day_id_mode', ragged=False, dtype=tf.int32, shape=(None,))        \n",
    "    inputs['part_of_day_code'] = tf.keras.Input(name='part_of_day_code', ragged=False, dtype=tf.int32, shape=(None,))        \n",
    "    inputs['n_days_any_url'] = tf.keras.Input(name='n_days_any_url', ragged=False, dtype=tf.int32, shape=())\n",
    "    print(inputs)\n",
    "    return inputs\n",
    "\n",
    "def create_encoder_model(num_layers, num_heads, dropout, url_host_id_emb_dim):\n",
    "    print(f'create_encoder_model: {num_layers=} {num_heads=} {dropout=} {url_host_id_emb_dim=}')\n",
    "    inputs = get_encoder_inputs()\n",
    "       \n",
    "    print('Prepare mask')\n",
    "    mask = inputs['url_host_id'] != 0\n",
    "\n",
    "    print('Create url_host_id embedding')\n",
    "    url_host_id_embedding = tf.keras.layers.Embedding(\n",
    "        input_dim=enconding_config.vocabulary_size_full, # 0-pad, 1-oof, 2-mask, 2-start, 3-end, ....-tokens\n",
    "        output_dim=url_host_id_emb_dim,\n",
    "        mask_zero=True,\n",
    "        name='url_host_id_embedding',\n",
    "    )(inputs['url_host_id'])   \n",
    "    \n",
    "    ###########################################################################\n",
    "    \n",
    "    print('Create part of day embedding')\n",
    "    part_of_day_id_mode_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=4+1,\n",
    "            output_dim=url_host_id_emb_dim,\n",
    "            mask_zero=True,\n",
    "            name='part_of_day_id_mode_embedding',\n",
    "        )(inputs['part_of_day_id_mode'] + 1)    \n",
    "    \n",
    "    print('Create part of day code embedding')\n",
    "    part_of_day_code_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=16+1,\n",
    "            output_dim=url_host_id_emb_dim,\n",
    "            mask_zero=False,\n",
    "            name='part_of_day_code_embedding',\n",
    "        )(inputs['part_of_day_code']+1)       \n",
    "    \n",
    "    \n",
    "    ###########################################################################\n",
    "    \n",
    "    print('Create request_cnt_total_vs_n_days_url embedding')\n",
    "    request_cnt_total = tf.cast(inputs['request_cnt_total'],tf.float32)     \n",
    "    n_days_url = tf.cast(inputs['n_days_url'],tf.float32)\n",
    "    request_cnt_total_vs_n_days_url = request_cnt_total /n_days_url\n",
    "    \n",
    "    request_cnt_total_vs_n_days_url = get_discretization_layer(\n",
    "        data=url_stat_by_user['request_cnt_total'].to_numpy() / url_stat_by_user['n_days_url'].to_numpy(),\n",
    "        n_bins=100,\n",
    "        name='request_cnt_total_vs_n_days_url_bins'\n",
    "    )(request_cnt_total_vs_n_days_url)\n",
    "    \n",
    "    request_cnt_total_vs_n_days_url = tf.where(request_cnt_total != -1, request_cnt_total_vs_n_days_url+1, tf.zeros_like(request_cnt_total_vs_n_days_url))\n",
    "    \n",
    "    request_cnt_total_vs_n_days_url_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=31+1,\n",
    "            output_dim=url_host_id_emb_dim,\n",
    "            mask_zero=True,\n",
    "            name='request_cnt_total_vs_n_days_url_embedding',\n",
    "        )(request_cnt_total_vs_n_days_url)  \n",
    "    print(f'{request_cnt_total_vs_n_days_url_embedding.shape=}')\n",
    "    \n",
    "    print('Create request_cnt_total_vs_n_days_any_url embedding')\n",
    "    n_days_any_url = tf.cast(inputs['n_days_any_url'],tf.float32)\n",
    "    n_days_any_url = tf.expand_dims(n_days_any_url, axis=-1)\n",
    "    request_cnt_total_vs_n_days_any_url = request_cnt_total / n_days_any_url\n",
    "    \n",
    "    request_cnt_total_vs_n_days_any_url = get_discretization_layer(\n",
    "        data=url_stat_by_user['request_cnt_total'].to_numpy() / url_stat_by_user['n_days_any_url'].to_numpy(), \n",
    "        n_bins=32,\n",
    "        name='request_cnt_total_vs_n_days_any_url_bins'\n",
    "    )(request_cnt_total_vs_n_days_any_url)\n",
    "    \n",
    "    request_cnt_total_vs_n_days_any_url = tf.where(request_cnt_total != -1, request_cnt_total_vs_n_days_any_url+1, tf.zeros_like(request_cnt_total_vs_n_days_any_url))\n",
    "        \n",
    "    request_cnt_total_vs_n_days_any_url_embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=32+1,\n",
    "            output_dim=url_host_id_emb_dim,\n",
    "            mask_zero=True,\n",
    "            name='request_cnt_total_vs_n_days_any_url_embedding',\n",
    "        )(request_cnt_total_vs_n_days_any_url)      \n",
    "    print(f'{request_cnt_total_vs_n_days_any_url_embedding.shape=}')\n",
    "    \n",
    "    ###########################################################################\n",
    "    outputs = url_host_id_embedding + request_cnt_total_vs_n_days_url_embedding + request_cnt_total_vs_n_days_any_url_embedding  + part_of_day_id_mode_embedding + part_of_day_code_embedding\n",
    "    ###########################################################################   \n",
    "    \n",
    "    print('Apply layer normalization and dropout to the embedding.')\n",
    "    outputs = tf.keras.layers.LayerNormalization(epsilon=NORM_EPSILON)(outputs)\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)    \n",
    "    outputs._keras_mask = mask    \n",
    "\n",
    "    # Add a number of encoder blocks\n",
    "    for i in range(num_layers):\n",
    "        outputs = keras_nlp.layers.TransformerEncoder(\n",
    "            intermediate_dim=url_host_id_emb_dim*2,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            layer_norm_epsilon=NORM_EPSILON,\n",
    "        )(outputs)  # , padding_mask=mask\n",
    "        \n",
    "    encoder_model = tf.keras.Model(inputs, outputs, name='encoder_model')\n",
    "    return encoder_model\n",
    "\n",
    "def test():\n",
    "    encoder_model = create_encoder_model(\n",
    "        num_layers=1,\n",
    "        num_heads=4,\n",
    "        dropout=0.1,\n",
    "        url_host_id_emb_dim=64,\n",
    "    )\n",
    "    encoder_model.summary()    \n",
    "    tf.keras.utils.plot_model(encoder_model, to_file=\"encoder_model.png\", rankdir='LR') # , show_shapes=True, show_dtype=True\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d62ec0",
   "metadata": {
    "cellId": "y9sh4u1h3pvs6qb9ppy8d"
   },
   "outputs": [],
   "source": [
    "class FinalMetricCallback(tf.keras.callbacks.Callback):\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        logs['is_male_gini'] = (2*logs[\"is_male_auc\"]-1)\n",
    "        logs['metric'] = 2*logs[\"age_bins_f1\"] + (2*logs[\"is_male_auc\"]-1)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs['is_male_gini'] = (2*logs[\"is_male_auc\"]-1)\n",
    "        logs['metric'] = 2*logs[\"age_bins_f1\"] + (2*logs[\"is_male_auc\"]-1)\n",
    "        logs['val_is_male_gini'] = (2*logs[\"val_is_male_auc\"]-1)\n",
    "        logs['val_metric'] = 2*logs[\"val_age_bins_f1\"] + (2*logs[\"val_is_male_auc\"]-1)\n",
    "        \n",
    "def get_callbacks(model_name):\n",
    "    return [\n",
    "        FinalMetricCallback(),\n",
    "        tf.keras.callbacks.CSVLogger(get_model_report_path(model_name)),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath = get_model_checkpoint_path(model_name),   \n",
    "            verbose=1,\n",
    "            save_best_only=False,\n",
    "            save_weights_only=True,\n",
    "            save_freq='epoch'),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac2edde",
   "metadata": {
    "cellId": "d5wpzhfwjfmvcw3tlf7er8"
   },
   "outputs": [],
   "source": [
    "user_id_embedding_size = 256\n",
    "\n",
    "@logger\n",
    "def get_static_features(inputs):\n",
    "    user_embedding = load_embedding('user_id_from_user_id_vs_url_host_id2', user_id_embedding_size)\n",
    "    \n",
    "    user_id = tf.keras.layers.Embedding(\n",
    "        user_id_count, user_id_embedding_size, name='user_id_embedding',\n",
    "        embeddings_initializer=tf.keras.initializers.Constant(user_embedding),\n",
    "        trainable=False\n",
    "    )(inputs['user_id'])\n",
    "            \n",
    "    region_name_id = tf.keras.layers.Embedding(81, 16, input_shape=(), name='region_name_id_embending')(inputs['region_name_id_primary'])\n",
    "    city_name_id = tf.keras.layers.Embedding(985, 64, input_shape=(), name='city_name_id_embending')(inputs['city_name_id_primary'])\n",
    "    cpe_manufacturer_name_id = tf.keras.layers.Embedding(37, 8, input_shape=(), name='cpe_manufacturer_name_id_embending')(inputs['cpe_manufacturer_name_id_primary'])\n",
    "    cpe_model_name_id = tf.keras.layers.Embedding(599, 64, input_shape=(), name='cpe_model_name_id_embending')(inputs['cpe_model_name_id_primary'])\n",
    "    cpe_type_cd_id = tf.keras.layers.Embedding(4, 2, input_shape=(), name='cpe_type_cd_id_embending')(inputs['cpe_type_cd_id_primary'])\n",
    "    cpe_model_os_type_id = tf.keras.layers.Embedding(3, 2, input_shape=(), name='cpe_model_os_type_id_embending')(inputs['cpe_model_os_type_id_primary'])   \n",
    "    region_name_id_count = get_normalization_layer(data=cat_features['region_name_id_count'].to_numpy(), name='region_name_id_count_bins')(inputs['region_name_id_count'])\n",
    "    city_name_id_count = get_normalization_layer(data=cat_features['city_name_id_count'].to_numpy(), name='city_name_id_count_bins')(inputs['city_name_id_count'])\n",
    "    \n",
    "    n_price_bins=32\n",
    "    price = get_discretization_layer(data=price_features['price'].to_numpy(), n_bins=n_price_bins, name='price_bins')(inputs['price'])\n",
    "    price = tf.keras.layers.Embedding(n_price_bins, 8, input_shape=(), name='price_embending')(price)    \n",
    "\n",
    "    static_features_list = [\n",
    "        user_id,\n",
    "        region_name_id, city_name_id, cpe_manufacturer_name_id, cpe_model_name_id, cpe_type_cd_id, cpe_model_os_type_id, \n",
    "        region_name_id_count, city_name_id_count, \n",
    "        price,\n",
    "    ]\n",
    "    static_features = tf.concat(static_features_list, axis=-1)   \n",
    "    static_features = tf.keras.layers.LayerNormalization(epsilon=NORM_EPSILON)(static_features)    \n",
    "    return static_features\n",
    "\n",
    "@logger\n",
    "def create_full_model(encoder_model, fc_hidden_units, male_fc_hidden_units, age_fc_hidden_units, fc_dropout_rate,):\n",
    "    print(f'create_full_model: {fc_hidden_units=} {male_fc_hidden_units=} {age_fc_hidden_units=} {fc_dropout_rate=}')\n",
    "    \n",
    "    ###########################################################################\n",
    "    inputs = get_encoder_inputs()\n",
    "    encoder_outputs = encoder_model(inputs)\n",
    "    encoder_outputs1 = tf.keras.layers.GlobalAveragePooling1D()(encoder_outputs)\n",
    "    encoder_outputs2 = tf.keras.layers.GlobalMaxPool1D()(encoder_outputs)\n",
    "\n",
    "    ###########################################################################\n",
    "    inputs['user_id'] = tf.keras.Input(name='user_id', dtype=tf.int32, shape=())    \n",
    "    inputs['region_name_id_primary'] = tf.keras.Input(name='region_name_id_primary', dtype=tf.int32, shape=())    \n",
    "    inputs['city_name_id_primary'] = tf.keras.Input(name='city_name_id_primary', dtype=tf.int32, shape=())    \n",
    "    inputs['cpe_manufacturer_name_id_primary'] = tf.keras.Input(name='cpe_manufacturer_name_id_primary', dtype=tf.int32, shape=())    \n",
    "    inputs['cpe_model_name_id_primary'] = tf.keras.Input(name='cpe_model_name_id_primary', dtype=tf.int32, shape=())    \n",
    "    inputs['cpe_type_cd_id_primary'] = tf.keras.Input(name='cpe_type_cd_id_primary', dtype=tf.int32, shape=())    \n",
    "    inputs['cpe_model_os_type_id_primary'] = tf.keras.Input(name='cpe_model_os_type_id_primary', dtype=tf.int32, shape=())  \n",
    "    inputs['region_name_id_count'] = tf.keras.Input(name='region_name_id_count', dtype=tf.int32, shape=(1))  \n",
    "    inputs['city_name_id_count'] = tf.keras.Input(name='city_name_id_count', dtype=tf.int32, shape=(1))  \n",
    "    inputs['cpe_model_name_id_count'] = tf.keras.Input(name='cpe_model_name_id_count', dtype=tf.int32, shape=(1))   ###### !!!!!!!!!!!! Не используется\n",
    "    inputs['url_host_id_count'] = tf.keras.Input(name='url_host_id_count', dtype=tf.int32, shape=(1))               ###### !!!!!!!!!!!! Не используется\n",
    "    inputs['price'] = tf.keras.Input(name='price', dtype=tf.float32, shape=())    \n",
    "    \n",
    "    static_features = get_static_features(inputs)  \n",
    "    ###########################################################################\n",
    "\n",
    "    x = tf.concat([\n",
    "        encoder_outputs1, \n",
    "        encoder_outputs2, \n",
    "        static_features\n",
    "    ], axis=-1)\n",
    "    \n",
    "    # Fully-connected layers block\n",
    "    for layer_id, num_units in enumerate(fc_hidden_units):\n",
    "        x = tf.keras.layers.Dense(num_units, name=f'final_dense_{layer_id}_{num_units}')(x)\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.LeakyReLU()(x)\n",
    "        x = tf.keras.layers.Dropout(fc_dropout_rate)(x)   \n",
    "        \n",
    "    male = x\n",
    "    for layer_id, num_units in enumerate(male_fc_hidden_units):\n",
    "        male = tf.keras.layers.Dense(num_units, name=f'male_dense_{layer_id}_{num_units}')(male)\n",
    "        male = tf.keras.layers.BatchNormalization()(male)\n",
    "        male = tf.keras.layers.LeakyReLU()(male)\n",
    "        male = tf.keras.layers.Dropout(fc_dropout_rate)(male)            \n",
    "    male_output = tf.keras.layers.Dense(1, activation='sigmoid', name='is_male')(male)    # \n",
    "    \n",
    "    age = x\n",
    "    for layer_id, num_units in enumerate(age_fc_hidden_units):\n",
    "        age = tf.keras.layers.Dense(num_units, name=f'age_dense_{layer_id}_{num_units}')(age)\n",
    "        age = tf.keras.layers.BatchNormalization()(age)\n",
    "        age = tf.keras.layers.LeakyReLU()(age)\n",
    "        age = tf.keras.layers.Dropout(fc_dropout_rate)(age)      \n",
    "    age_bins_output = tf.keras.layers.Dense(6, activation='softmax', name='age_bins')(age)  # \n",
    "    age_output = tf.keras.layers.Dense(1, activation=None, name='age')(age) \n",
    "        \n",
    "    model = tf.keras.Model(inputs=inputs, outputs={\n",
    "        'is_male':male_output,\n",
    "        'age_bins':age_bins_output, \n",
    "        'age':age_output,\n",
    "    }, name='full_model')\n",
    "    return model\n",
    "\n",
    "def test():\n",
    "    encoder_num_layers = 1\n",
    "    encoder_num_heads = 3\n",
    "    encoder_dropout=0.1\n",
    "    encoder_url_host_id_emb_dim = 64\n",
    "\n",
    "    fc_hidden_units = [256,128]\n",
    "    male_fc_hidden_units = [128, 64]\n",
    "    age_fc_hidden_units = [128, 64]\n",
    "    fc_dropout_rate = 0.2\n",
    "\n",
    "    learning_rate = 1e-4\n",
    "    batch_size = 64\n",
    "\n",
    "    encoder_model = create_encoder_model(\n",
    "        num_layers=encoder_num_layers,\n",
    "        num_heads=encoder_num_heads,\n",
    "        dropout=encoder_dropout,\n",
    "        url_host_id_emb_dim=encoder_url_host_id_emb_dim,\n",
    "    )\n",
    "    tf.keras.utils.plot_model(encoder_model, to_file=\"encoder_model.png\", rankdir='LR') # , show_shapes=True, show_dtype=True\n",
    "    \n",
    "    model = create_full_model(\n",
    "        encoder_model, \n",
    "        fc_hidden_units, \n",
    "        male_fc_hidden_units,\n",
    "        age_fc_hidden_units,\n",
    "        fc_dropout_rate,\n",
    "    )\n",
    "    tf.keras.utils.plot_model(model, to_file=\"full_model.png\", rankdir='LR')\n",
    "    model.summary()\n",
    "    \n",
    "    print(model.predict(get_train_dataset(train_sample, batch_size=2).take(2)))\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d67337",
   "metadata": {
    "cellId": "ui571auci0ljdk5l5z48d"
   },
   "source": [
    "# Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004c7644",
   "metadata": {
    "cellId": "pyxodoufdqxlzx1pim5pj"
   },
   "outputs": [],
   "source": [
    "PREDICTIONS_PER_SEQ = 32\n",
    "\n",
    "@logger\n",
    "def get_lm_masked_dataset(dataset, mask_selection_rate=0.2, mask_token_rate=0.8, random_token_rate=0.1):\n",
    "    \n",
    "    # https://keras.io/api/keras_nlp/preprocessing_layers/masked_lm_mask_generator/\n",
    "    masker = keras_nlp.layers.MaskedLMMaskGenerator(\n",
    "        vocabulary_size=enconding_config.vocabulary_size_full,\n",
    "        mask_selection_rate=mask_selection_rate,\n",
    "        mask_token_id=enconding_config.mask_token_id,\n",
    "        mask_selection_length=PREDICTIONS_PER_SEQ,\n",
    "        unselectable_token_ids=[enconding_config.pad_token_id, enconding_config.start_token_id, enconding_config.end_token_id],\n",
    "        mask_token_rate=mask_token_rate,\n",
    "        random_token_rate=random_token_rate,\n",
    "    )\n",
    "\n",
    "    def fn(inputs, target):\n",
    "        outputs = masker(inputs['url_host_id'])\n",
    "        inputs['url_host_id'] = outputs[\"token_ids\"]\n",
    "        inputs['mask_positions'] = outputs[\"mask_positions\"]\n",
    "\n",
    "        # Split the masking layer outputs into a (features, labels, and weights) tuple that we can use with keras.Model.fit().\n",
    "        labels = outputs[\"mask_ids\"]#.to_tensor()\n",
    "        weights = outputs[\"mask_weights\"]#.to_tensor()\n",
    "        return inputs, labels, weights\n",
    "    \n",
    "    dataset = dataset.map(fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)    \n",
    "    return dataset\n",
    "\n",
    "pretrain_train, pretrain_valid = get_pretrain_train_valid_df()\n",
    "pretrain_train = pretrain_train[:10]\n",
    "pretrain_valid = pretrain_valid[:10]\n",
    "\n",
    "for i in get_lm_masked_dataset(get_train_dataset(pretrain_train, batch_size=2)).take(1):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cd5895",
   "metadata": {
    "cellId": "redrhtpr8lmhu0vr8s563u"
   },
   "outputs": [],
   "source": [
    "@logger\n",
    "def create_pretrain_model(encoder_model):\n",
    "    inputs = get_encoder_inputs()\n",
    "    encoder_outputs = encoder_model(inputs)    \n",
    "    \n",
    "    # Predict an output word for each masked input token.\n",
    "    # We use the input token embedding to project from our encoded vectors to\n",
    "    # vocabulary logits, which has been shown to improve training efficiency.\n",
    "    \n",
    "    inputs['mask_positions'] = tf.keras.Input(name='mask_positions', ragged=False, dtype=tf.int32, shape=(PREDICTIONS_PER_SEQ,))    \n",
    "    url_host_id_embedding = encoder_model.get_layer('url_host_id_embedding').embeddings\n",
    "    \n",
    "    masked_head_outputs = keras_nlp.layers.MaskedLMHead(\n",
    "        embedding_weights=url_host_id_embedding,\n",
    "#         activation=\"softmax\",\n",
    "        activation=None,\n",
    "    )(inputs=encoder_outputs, mask_positions=inputs['mask_positions'])\n",
    "    \n",
    "    pretraining_model = tf.keras.Model(inputs, masked_head_outputs, name='masked_model')\n",
    "    pretraining_model.summary()    \n",
    "    return pretraining_model\n",
    "\n",
    "def test():\n",
    "    encoder_num_layers = 4\n",
    "    encoder_num_heads = 4\n",
    "    encoder_dropout=0.1\n",
    "    encoder_url_host_id_emb_dim = 64\n",
    "\n",
    "    fc_hidden_units = [256,128]\n",
    "    male_fc_hidden_units = [128, 64]\n",
    "    age_fc_hidden_units = [128, 64]\n",
    "    fc_dropout_rate = 0.2\n",
    "\n",
    "    learning_rate = 1e-4\n",
    "    batch_size = 32\n",
    "\n",
    "    encoder_model = create_encoder_model(\n",
    "        num_layers=encoder_num_layers,\n",
    "        num_heads=encoder_num_heads,\n",
    "        dropout=encoder_dropout,\n",
    "        url_host_id_emb_dim=encoder_url_host_id_emb_dim,\n",
    "    )\n",
    "    \n",
    "    model = create_pretrain_model(encoder_model)\n",
    "    tf.keras.utils.plot_model(model, to_file=\"pretrain_model.png\", rankdir='LR')\n",
    "    model.summary()\n",
    "    \n",
    "    dataset = get_lm_masked_dataset(get_train_dataset(pretrain_train, batch_size=2)).take(1)\n",
    "    print(model.predict(dataset))\n",
    "    \n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e1f04d",
   "metadata": {
    "cellId": "3t2aztaed2jgku3hig8dl"
   },
   "outputs": [],
   "source": [
    "@logger\n",
    "def train_pretrain_model(\n",
    "    model_name, \n",
    "    encoder_num_layers, encoder_num_heads, encoder_dropout, encoder_url_host_id_emb_dim,\n",
    "    batch_size, learning_rate, epoch_max):\n",
    "    \n",
    "    print(f'train_pretrain_model: {model_name=} {batch_size=} {learning_rate=} {epoch_max=}')\n",
    "\n",
    "    encoder_model = create_encoder_model(\n",
    "        num_layers=encoder_num_layers,\n",
    "        num_heads=encoder_num_heads,\n",
    "        dropout=encoder_dropout,\n",
    "        url_host_id_emb_dim=encoder_url_host_id_emb_dim,\n",
    "    )\n",
    "    \n",
    "    model = create_pretrain_model(encoder_model)\n",
    "    model.summary()\n",
    "\n",
    "    pretrain_train, dataset_valid = get_pretrain_train_valid_df()\n",
    "    dataset_train = get_lm_masked_dataset(get_train_dataset(pretrain_train, batch_size=batch_size, data_rate=0.5)).shuffle(128).prefetch(tf.data.AUTOTUNE)\n",
    "    dataset_valid = get_lm_masked_dataset(get_valid_dataset(dataset_valid, batch_size=batch_size)).cache()\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.experimental.AdamW(learning_rate),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        weighted_metrics=tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "    )\n",
    "    \n",
    "    def get_callbacks(model_name):\n",
    "        return [\n",
    "            tf.keras.callbacks.CSVLogger(get_model_report_path(model_name)),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath = get_model_checkpoint_path(model_name),   \n",
    "                verbose=1,\n",
    "                save_best_only=False,\n",
    "                save_weights_only=True,\n",
    "                save_freq='epoch'),\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_sparse_categorical_accuracy',\n",
    "                mode='max',\n",
    "                verbose=1,    \n",
    "                patience=2,\n",
    "                restore_best_weights=True),\n",
    "        ]    \n",
    "    \n",
    "    model.fit(\n",
    "        x=dataset_train,\n",
    "        validation_data=dataset_valid, \n",
    "        epochs=epoch_max, \n",
    "        callbacks = get_callbacks(model_name)\n",
    "    )    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfacac3",
   "metadata": {
    "cellId": "f4x3au09z6mrysikfso20s"
   },
   "outputs": [],
   "source": [
    "@logger\n",
    "def train_pretrain_model_additional(\n",
    "    model_name, \n",
    "    encoder_num_layers, encoder_num_heads, encoder_dropout, encoder_url_host_id_emb_dim,\n",
    "    checkpoint_path,\n",
    "    batch_size, learning_rate, epoch_max,\n",
    "    data_rate=0.5):\n",
    "    \n",
    "    print(f'train_pretrain_model: {model_name=} {batch_size=} {learning_rate=} {epoch_max=}')\n",
    "\n",
    "    encoder_model = create_encoder_model(\n",
    "        num_layers=encoder_num_layers,\n",
    "        num_heads=encoder_num_heads,\n",
    "        dropout=encoder_dropout,\n",
    "        url_host_id_emb_dim=encoder_url_host_id_emb_dim,\n",
    "    )\n",
    "    \n",
    "    model = create_pretrain_model(encoder_model)\n",
    "    model.load_weights(checkpoint_path)\n",
    "    model.summary()\n",
    "\n",
    "    pretrain_train, dataset_valid = get_pretrain_train_valid_df()\n",
    "    dataset_train = get_lm_masked_dataset(get_train_dataset(pretrain_train, batch_size=batch_size, data_rate=data_rate))\n",
    "    dataset_valid = get_lm_masked_dataset(get_valid_dataset(dataset_valid, batch_size=batch_size)).cache()\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.experimental.AdamW(learning_rate),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        weighted_metrics=tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "    )\n",
    "    \n",
    "    def get_callbacks(model_name):\n",
    "        return [\n",
    "            tf.keras.callbacks.CSVLogger(get_model_report_path(model_name)),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath = get_model_checkpoint_path(model_name),   \n",
    "                verbose=1,\n",
    "                save_best_only=False,\n",
    "                save_weights_only=True,\n",
    "                save_freq='epoch'),\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_sparse_categorical_accuracy',\n",
    "                mode='max',\n",
    "                verbose=1,    \n",
    "                patience=2,\n",
    "                restore_best_weights=True),\n",
    "        ]    \n",
    "    \n",
    "    model.fit(\n",
    "        x=dataset_train,\n",
    "        validation_data=dataset_valid, \n",
    "        epochs=epoch_max, \n",
    "        callbacks = get_callbacks(model_name)\n",
    "    )    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4311b1db",
   "metadata": {
    "cellId": "44pafryqytrmfeq5gqx27m"
   },
   "source": [
    "## Обучим masked модель на коротких последовательностях (половину данных удаляем для скорости)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c0569f",
   "metadata": {
    "cellId": "6hyr6hg9wqxbwixf2fa9g"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_nlp\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "print(f'numpy={np.__version__}')\n",
    "print(f'pandas={pd.__version__}')\n",
    "print(f'polars={pl.__version__}')\n",
    "print(f'tf={tf.__version__}')\n",
    "print(f'keras_nlp={keras_nlp.__version__}')\n",
    "\n",
    "encoder_num_layers = 4\n",
    "encoder_num_heads = 4\n",
    "encoder_dropout=0.1\n",
    "encoder_url_host_id_emb_dim = 256\n",
    "\n",
    "learning_rate = 1e-4\n",
    "batch_size = 32\n",
    "    \n",
    "model_name = f'uhf08_pretarain_url_{encoder_url_host_id_emb_dim}_tf_{encoder_num_layers}_{encoder_num_heads}_adam_{learning_rate}_b_{batch_size}_v11'\n",
    "print(f'{model_name=}') \n",
    "    \n",
    "train_pretrain_model(\n",
    "    model_name=model_name, \n",
    "    encoder_num_layers=encoder_num_layers, encoder_num_heads=encoder_num_heads, encoder_dropout=encoder_dropout, encoder_url_host_id_emb_dim=encoder_url_host_id_emb_dim, \n",
    "    batch_size=batch_size, learning_rate=learning_rate, epoch_max=30, data_rate=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68621142",
   "metadata": {
    "cellId": "k89u3apu1akt615rmye4"
   },
   "source": [
    "## Дообучим masked модель на полных последовательностях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f11a13a",
   "metadata": {
    "cellId": "dmp94t0pzr5zngxqu26sv"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_nlp\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "print(f'numpy={np.__version__}')\n",
    "print(f'pandas={pd.__version__}')\n",
    "print(f'polars={pl.__version__}')\n",
    "print(f'tf={tf.__version__}')\n",
    "print(f'keras_nlp={keras_nlp.__version__}')\n",
    "\n",
    "encoder_num_layers = 4\n",
    "encoder_num_heads = 4\n",
    "encoder_dropout=0.1\n",
    "encoder_url_host_id_emb_dim = 256\n",
    "\n",
    "learning_rate = 1e-4\n",
    "batch_size = 32\n",
    "    \n",
    "model_name = f'uhf08_pretarain_url_{encoder_url_host_id_emb_dim}_tf_{encoder_num_layers}_{encoder_num_heads}_adam_{learning_rate}_b_{batch_size}_v12'\n",
    "print(f'{model_name=}') \n",
    "    \n",
    "train_pretrain_model_additional(\n",
    "    model_name=model_name, \n",
    "    encoder_num_layers=encoder_num_layers, encoder_num_heads=encoder_num_heads, encoder_dropout=encoder_dropout, encoder_url_host_id_emb_dim=encoder_url_host_id_emb_dim, \n",
    "    checkpoint_path=get_model_checkpoint_path_by_epoch(uhf08_pretarain_url_256_tf_4_4_adam_0.0001_b_32_v11, 10),\n",
    "    batch_size=batch_size, learning_rate=learning_rate, epoch_max=30, data_rate=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f20a30",
   "metadata": {
    "cellId": "jtz7bp98zyk346i4lxkprs"
   },
   "source": [
    "## Дообучим masked модель на полных последовательностях с меньшим learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d956b451",
   "metadata": {
    "cellId": "wbj11mthmu702j9tg73g4"
   },
   "outputs": [],
   "source": [
    "#!g1.1\n",
    "#pragma async\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras_nlp\n",
    "\n",
    "from typing import Dict, Text\n",
    "\n",
    "print(f'numpy={np.__version__}')\n",
    "print(f'pandas={pd.__version__}')\n",
    "print(f'polars={pl.__version__}')\n",
    "print(f'tf={tf.__version__}')\n",
    "print(f'keras_nlp={keras_nlp.__version__}')\n",
    "\n",
    "encoder_num_layers = 4\n",
    "encoder_num_heads = 4\n",
    "encoder_dropout=0.1\n",
    "encoder_url_host_id_emb_dim = 256\n",
    "\n",
    "learning_rate = 3e-5\n",
    "batch_size = 32\n",
    "    \n",
    "model_name = f'uhf08_pretarain_url_{encoder_url_host_id_emb_dim}_tf_{encoder_num_layers}_{encoder_num_heads}_adam_{learning_rate}_b_{batch_size}_v13'\n",
    "print(f'{model_name=}') \n",
    "    \n",
    "train_pretrain_model_additional(\n",
    "    model_name=model_name, \n",
    "    encoder_num_layers=encoder_num_layers, encoder_num_heads=encoder_num_heads, encoder_dropout=encoder_dropout, encoder_url_host_id_emb_dim=encoder_url_host_id_emb_dim, \n",
    "    checkpoint_path='/home/jupyter/mnt/s3/mtsmlcup/models/uhf08_pretarain_url_256_tf_4_4_adam_0.0001_b_32_v12/11_checkpoint',\n",
    "    batch_size=batch_size, learning_rate=learning_rate, epoch_max=30, data_rate=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec9fdd3",
   "metadata": {
    "cellId": "60jc9p8775653mjdxmi8b2"
   },
   "source": [
    "# Строим модель на базе предобученного encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eef672c",
   "metadata": {
    "cellId": "npl3duccgx9c2i1oj1klph"
   },
   "source": [
    "## Определяем логику обучеения\n",
    "* сначала с большим learning_rate, но заблокированным для обучения encoder\n",
    "* продолжаем с малым learning_rate и разблокированным для обучения encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ec2680",
   "metadata": {
    "cellId": "3jipvs4snxw5v4xsco7cag"
   },
   "outputs": [],
   "source": [
    "@logger\n",
    "def train_full_model_from_pretrain(\n",
    "    model_name, cv_fold_id, \n",
    "    encoder_num_layers, encoder_num_heads, encoder_dropout, encoder_url_host_id_emb_dim,    \n",
    "    prepratin_model_name, prepratin_model_epoch, \n",
    "    fc_hidden_units, male_fc_hidden_units, age_fc_hidden_units,fc_dropout_rate,\n",
    "    learning_rate1, epoch_max1,\n",
    "    learning_rate2, epoch_max2,\n",
    "    batch_size, data_rate):\n",
    "    \n",
    "    print(f'train_full_model_from_pretrain: {model_name=} {learning_rate1=} {epoch_max1=} {learning_rate2=} {epoch_max2=} {prepratin_model_name=} {prepratin_model_epoch=}')\n",
    "    \n",
    "    ###########################################################################\n",
    "    # Вспомогательные функции \n",
    "    \n",
    "    def get_callbacks(model_name):\n",
    "        return [\n",
    "            FinalMetricCallback(),\n",
    "            tf.keras.callbacks.CSVLogger(get_model_report_path(model_name), append=True),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath = get_model_checkpoint_path(model_name),   \n",
    "                verbose=1,\n",
    "                save_best_only=False,\n",
    "                save_weights_only=True,\n",
    "                save_freq='epoch'),\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor='val_metric',\n",
    "                mode='max',\n",
    "                verbose=1,    \n",
    "                patience=3,\n",
    "                restore_best_weights=True),\n",
    "        ]\n",
    "    \n",
    "    # Загрузка данных\n",
    "    dataset_train = get_train_dataset(get_data_by_target(cv_folds.get_train_target_fold(cv_fold_id)), batch_size=batch_size, data_rate=data_rate)\n",
    "    dataset_valid = get_valid_dataset(get_data_by_target(cv_folds.get_valid_target_fold(cv_fold_id)), batch_size=batch_size) \n",
    "    \n",
    "    ###########################################################################\n",
    "    # Создаем модеель на основании предзагруженной\n",
    "    \n",
    "    encoder_model = create_encoder_model(\n",
    "        num_layers=encoder_num_layers,\n",
    "        num_heads=encoder_num_heads,\n",
    "        dropout=encoder_dropout,\n",
    "        url_host_id_emb_dim=encoder_url_host_id_emb_dim,\n",
    "    )\n",
    "    encoder_model.summary()\n",
    "    \n",
    "    pretrain_model = create_pretrain_model(encoder_model)    \n",
    "    checkpoint_path = get_model_checkpoint_path_by_epoch(prepratin_model_name, prepratin_model_epoch)    \n",
    "    pretrain_model.load_weights(checkpoint_path)     \n",
    "    \n",
    "    model = create_full_model(\n",
    "        encoder_model, \n",
    "        fc_hidden_units, \n",
    "        male_fc_hidden_units,\n",
    "        age_fc_hidden_units,\n",
    "        fc_dropout_rate,\n",
    "    )\n",
    "    \n",
    "    ###########################################################################\n",
    "    # Обучаем все, кроме encoder_model\n",
    "    \n",
    "    encoder_model.trainable = False\n",
    "    model.summary()\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.experimental.AdamW(learning_rate1),\n",
    "        loss_weights={\n",
    "            'is_male':1,\n",
    "            'age_bins':5,\n",
    "            'age':0.01, # 0.05\n",
    "        },\n",
    "        loss={\n",
    "            'is_male':tf.keras.losses.BinaryCrossentropy(), #tf.keras.losses.BinaryFocalCrossentropy(), # #\n",
    "            'age_bins':get_categorical_focal_crossentropy_loss(gamma=2.0, alpha=0.25),\n",
    "            'age':tf.keras.losses.Huber(),\n",
    "        },\n",
    "        metrics= {\n",
    "            'is_male':tf.keras.metrics.AUC(name='auc'),\n",
    "            'age_bins':tfa.metrics.F1Score(average='weighted', num_classes=6, name='f1'),\n",
    "            'age':tf.keras.metrics.RootMeanSquaredError(name='rmse')\n",
    "        },\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        x=dataset_train,\n",
    "        validation_data=dataset_valid.cache(), \n",
    "        epochs=epoch_max1, \n",
    "        callbacks = get_callbacks(model_name)\n",
    "    ) \n",
    "    \n",
    "    ###########################################################################\n",
    "    # Обучаем все\n",
    "    \n",
    "    encoder_model.trainable = True\n",
    "    model.summary()    \n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.experimental.AdamW(learning_rate2),\n",
    "        loss_weights={\n",
    "            'is_male':1,\n",
    "            'age_bins':5,\n",
    "            'age':0.01, # 0.05\n",
    "        },\n",
    "        loss={\n",
    "            'is_male':tf.keras.losses.BinaryCrossentropy(), #tf.keras.losses.BinaryFocalCrossentropy(), # #\n",
    "            'age_bins':get_categorical_focal_crossentropy_loss(gamma=2.0, alpha=0.25),\n",
    "            'age':tf.keras.losses.Huber(),\n",
    "        },\n",
    "        metrics= {\n",
    "            'is_male':tf.keras.metrics.AUC(name='auc'),\n",
    "            'age_bins':tfa.metrics.F1Score(average='weighted', num_classes=6, name='f1'),\n",
    "            'age':tf.keras.metrics.RootMeanSquaredError(name='rmse')\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        x=dataset_train,\n",
    "        validation_data=dataset_valid.cache(), \n",
    "        initial_epoch = epoch_max1,\n",
    "        epochs=epoch_max1+epoch_max2, \n",
    "        callbacks = get_callbacks(model_name)\n",
    "    ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa69aa4",
   "metadata": {
    "cellId": "fabwxlysgr60r0huj3gs5l"
   },
   "source": [
    "## Обучаем 5 моделей (CV5)\n",
    "Распределение данных очень нестабильное:\n",
    "* Много элементов, которые были у очень небольшого количества данных\n",
    "* Даты из разных интервалов, количество пользователей между датами сильно отличаеется\n",
    "\n",
    "В результате модель построенная по всем данным проигрывает устредненным предсказаниям 5 моделей, построенных по 80% данных (CV5)\n",
    "Более того, усреднение нескольких моделей с одинаковой архитектурой и построеенных по одним данным тоже дает прирос.\n",
    "Так же усреднение двух лучших epoch дает результат лучше каждого из них.\n",
    "Итоговый результат - усреднение предсказаний 5 моделей на 80% данных (CV5) * посчитанно 5 раз * предсказания с двух лучших epoch (25, 26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d5113e",
   "metadata": {
    "cellId": "pelgmn9yhq191fnn7fcx3"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from typing import Dict, Text\n",
    "import keras_nlp\n",
    "\n",
    "print(f'numpy={np.__version__}')\n",
    "print(f'pandas={pd.__version__}')\n",
    "print(f'polars={pl.__version__}')\n",
    "print(f'tf={tf.__version__}')\n",
    "print(f'tfa={tfa.__version__}')\n",
    "print(f'keras_nlp={keras_nlp.__version__}')\n",
    "\n",
    "encoder_num_layers = 4\n",
    "encoder_num_heads = 4\n",
    "encoder_dropout=0.1\n",
    "encoder_url_host_id_emb_dim = 256\n",
    "\n",
    "fc_hidden_units = [256,128]\n",
    "male_fc_hidden_units = [128, 64]\n",
    "age_fc_hidden_units = [128, 64]\n",
    "fc_dropout_rate = 0.2\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "learning_rate1=1e-4\n",
    "learning_rate2=1e-5\n",
    "    \n",
    "for cv_fold_id in range(5):\n",
    "    model_name = f'uhf08_pretrained_url_{encoder_url_host_id_emb_dim}_tf_{encoder_num_layers}_{encoder_num_heads}_adamw_{learning_rate1}_{learning_rate2}_b_{batch_size}_v13_01_08_cv5_{cv_fold_id}'\n",
    "    print(f'{model_name=}') \n",
    "    \n",
    "    train_full_model_from_pretrain(\n",
    "        model_name=model_name, cv_fold_id=cv_fold_id, \n",
    "        prepratin_model_name='uhf08_pretarain_url_256_tf_4_4_adam_3e-05_b_32_v13', prepratin_model_epoch=1, \n",
    "        encoder_num_layers=encoder_num_layers, encoder_num_heads=encoder_num_heads, encoder_dropout=encoder_dropout, encoder_url_host_id_emb_dim=encoder_url_host_id_emb_dim, \n",
    "        fc_hidden_units=fc_hidden_units, male_fc_hidden_units=male_fc_hidden_units, age_fc_hidden_units=age_fc_hidden_units, fc_dropout_rate=fc_dropout_rate,\n",
    "        batch_size=batch_size, \n",
    "        learning_rate1=learning_rate1, epoch_max1=20,\n",
    "        learning_rate2=learning_rate2, epoch_max2=20,\n",
    "        data_rate=1.0 # в предыдущих было 0.75\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77607b5f",
   "metadata": {
    "cellId": "oe4cmfymxkgg02b7qh9uyt"
   },
   "source": [
    "## Предсказываем предобученной моделью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e96a89c",
   "metadata": {
    "cellId": "dkj3cvn8ujca9l3j35wa66"
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "ModelCheckpoint = namedtuple('ModelCheckpoint', ['cv_fold_id', 'model_name', 'epoch'])\n",
    "\n",
    "CONFIG_PREDICTIONS_PATH = '/home/jupyter/mnt/s3/mtsmlcup/predictions'\n",
    "\n",
    "@logger\n",
    "def predict_cv_model_list():\n",
    "    # Create empty model\n",
    "    encoder_num_layers = 4\n",
    "    encoder_num_heads = 4\n",
    "    encoder_dropout=0.1\n",
    "    encoder_url_host_id_emb_dim = 256\n",
    "\n",
    "    fc_hidden_units = [256,128]\n",
    "    male_fc_hidden_units = [128, 64]\n",
    "    age_fc_hidden_units = [128, 64]\n",
    "    fc_dropout_rate = 0.2\n",
    "\n",
    "    batch_size = 64\n",
    "\n",
    "    encoder_model = create_encoder_model(\n",
    "        num_layers=encoder_num_layers,\n",
    "        num_heads=encoder_num_heads,\n",
    "        dropout=encoder_dropout,\n",
    "        url_host_id_emb_dim=encoder_url_host_id_emb_dim,\n",
    "    )\n",
    "    encoder_model.summary()\n",
    "    \n",
    "    model = create_full_model(\n",
    "        encoder_model, \n",
    "        fc_hidden_units, \n",
    "        male_fc_hidden_units,\n",
    "        age_fc_hidden_units,\n",
    "        fc_dropout_rate,\n",
    "    )\n",
    "    model.summary()\n",
    "    \n",
    "    # Load test data\n",
    "    test_data = get_data_by_target(cv_folds.get_test_target())\n",
    "    test_dataset = get_valid_dataset(test_data, batch_size=128)\n",
    "    \n",
    "    # Define list of models\n",
    "    model_checkpoint_list = [\n",
    "        ModelCheckpoint(cv_fold_id=0, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_08_cv5_0', epoch=24),\n",
    "        ModelCheckpoint(cv_fold_id=0, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_08_cv5_0', epoch=25),\n",
    "        ModelCheckpoint(cv_fold_id=0, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_09_cv5_0', epoch=24),\n",
    "        ModelCheckpoint(cv_fold_id=0, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_09_cv5_0', epoch=25),        \n",
    "        ModelCheckpoint(cv_fold_id=0, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_10_cv5_0', epoch=24),\n",
    "        ModelCheckpoint(cv_fold_id=0, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_10_cv5_0', epoch=25),\n",
    "        \n",
    "        ModelCheckpoint(cv_fold_id=1, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_08_cv5_1', epoch=24),\n",
    "        ModelCheckpoint(cv_fold_id=1, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_08_cv5_1', epoch=25),\n",
    "        ModelCheckpoint(cv_fold_id=1, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_09_cv5_1', epoch=24),\n",
    "        ModelCheckpoint(cv_fold_id=1, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_09_cv5_1', epoch=25),        \n",
    "        ModelCheckpoint(cv_fold_id=1, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_10_cv5_1', epoch=24),\n",
    "        ModelCheckpoint(cv_fold_id=1, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_10_cv5_1', epoch=25),    \n",
    "        \n",
    "        ModelCheckpoint(cv_fold_id=2, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_08_cv5_2', epoch=24),\n",
    "        ModelCheckpoint(cv_fold_id=2, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_08_cv5_2', epoch=25),\n",
    "        ModelCheckpoint(cv_fold_id=2, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_09_cv5_2', epoch=24),\n",
    "        ModelCheckpoint(cv_fold_id=2, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_09_cv5_2', epoch=25),        \n",
    "        ModelCheckpoint(cv_fold_id=2, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_10_cv5_2', epoch=24),\n",
    "        ModelCheckpoint(cv_fold_id=2, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_10_cv5_2', epoch=25),     \n",
    "        \n",
    "        ModelCheckpoint(cv_fold_id=3, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_08_cv5_3', epoch=24),\n",
    "        ModelCheckpoint(cv_fold_id=3, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_08_cv5_3', epoch=25),\n",
    "        ModelCheckpoint(cv_fold_id=3, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_09_cv5_3', epoch=24),\n",
    "        ModelCheckpoint(cv_fold_id=3, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_09_cv5_3', epoch=25),        \n",
    "        ModelCheckpoint(cv_fold_id=3, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_10_cv5_3', epoch=24),\n",
    "        ModelCheckpoint(cv_fold_id=3, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_10_cv5_3', epoch=25),     \n",
    "        \n",
    "        ModelCheckpoint(cv_fold_id=4, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_08_cv5_4', epoch=24),\n",
    "        ModelCheckpoint(cv_fold_id=4, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_08_cv5_4', epoch=25),\n",
    "        ModelCheckpoint(cv_fold_id=4, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_09_cv5_4', epoch=24),\n",
    "        ModelCheckpoint(cv_fold_id=4, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_09_cv5_4', epoch=25),        \n",
    "        ModelCheckpoint(cv_fold_id=4, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_10_cv5_4', epoch=24),\n",
    "        ModelCheckpoint(cv_fold_id=4, model_name='uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_10_cv5_4', epoch=25),             \n",
    "    ]    \n",
    "\n",
    "    for model_checkpoint in model_checkpoint_list:\n",
    "        print(f'{model_checkpoint=}')\n",
    "        model.load_weights(get_model_checkpoint_path_by_epoch(model_checkpoint.model_name, model_checkpoint.epoch)) \n",
    "        \n",
    "        pred = model.predict(test_dataset)\n",
    "        \n",
    "        def get_embedding_column(feature_name, column_name = None):\n",
    "            if column_name is None:\n",
    "                column_name = feature_name\n",
    "            print(f'get_embedding_column: {feature_name} -> {column_name}')\n",
    "            column = pl.Series(name=column_name, values=pred[feature_name].tolist())\n",
    "            return column\n",
    "        \n",
    "        prediction = test_data.select([\n",
    "                pl.col('user_id'),        \n",
    "                pl.col('is_male').alias('is_male_fact'),\n",
    "                pl.lit(pred['is_male'][:,0]).alias('is_male'),\n",
    "                pl.col('age_bins').alias('age_bins_fact'),\n",
    "                get_embedding_column('age_bins', 'age_pred'),\n",
    "                pl.lit(tf.argmax(pred['age_bins'], axis=-1).numpy()+1).alias('age'),\n",
    "            ])  \n",
    "\n",
    "        os.makedirs(CONFIG_PREDICTIONS_PATH, exist_ok=True)      \n",
    "        prediction_folder = Path(CONFIG_PREDICTIONS_PATH)     \n",
    "\n",
    "        print(f'predict_raw:is_male target distribution')\n",
    "        print(prediction['is_male'].describe())\n",
    "\n",
    "        print(f'predict_raw:age target distribution')\n",
    "        print(prediction.groupby('age').agg(pl.count()).sort('age'))        \n",
    "        assert ~any(prediction['age'] == 0)\n",
    "\n",
    "        prediction_path = prediction_folder / f'{model_checkpoint.model_name}_epoch_{model_checkpoint.epoch}.csv'\n",
    "        prediction_raw_path = prediction_folder / f'{model_checkpoint.model_name}_epoch_{model_checkpoint.epoch}_raw.parquet'\n",
    "        print(f'predict_raw:{prediction_path=}, {prediction_raw_path=}')\n",
    "\n",
    "        prediction.select(pl.col(['user_id', 'is_male', 'age'])).write_csv(prediction_path)        \n",
    "        prediction.write_parquet(prediction_raw_path)      \n",
    "            \n",
    "predict_cv_model_list() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c444b5",
   "metadata": {
    "cellId": "tgdsoarpxgr4p5907qno"
   },
   "source": [
    "## Усредняем предсказания нескольких моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dfe741",
   "metadata": {
    "cellId": "5v2gv60kj5iskayw3g4qpi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def save_prediction(prediction, name):\n",
    "    print(f'save_prediction:begin {name=}, {prediction.shape=}')\n",
    "    \n",
    "    prediction_folder = Path(CONFIG_PREDICTIONS_PATH)\n",
    "    os.makedirs(prediction_folder, exist_ok=True)    \n",
    "    \n",
    "    print(f'save_prediction:is_male target distribution')\n",
    "    print(prediction['is_male'].describe())\n",
    "\n",
    "    print(f'save_prediction:age target distribution')\n",
    "    print(prediction.groupby('age').agg(pl.count()).sort('age'))        \n",
    "    assert ~any(prediction['age'] == 0)\n",
    "    \n",
    "    prediction_path = prediction_folder / f'{name}.csv'\n",
    "    prediction_raw_path = prediction_folder / f'{name}_raw.parquet'\n",
    "    print(f'save_prediction:{prediction_path=}, {prediction_raw_path=}')\n",
    "    \n",
    "    prediction.select(pl.col(['user_id', 'is_male', 'age'])).write_csv(prediction_path)        \n",
    "    prediction.select(pl.col(['user_id', 'is_male', 'age', 'age_pred'])).write_parquet(prediction_raw_path)  \n",
    "    \n",
    "def get_raw_predictions(file_name_list):\n",
    "    prediction_folder = Path(CONFIG_PREDICTIONS_PATH)\n",
    "    \n",
    "    predictions_list = []\n",
    "    for file_name in file_name_list:\n",
    "        file_path = prediction_folder / file_name\n",
    "        print(file_path)\n",
    "        assert file_path.exists() == True    \n",
    "        predictions_list.append(pl.read_parquet(file_path))\n",
    "        \n",
    "    return pl.concat(predictions_list)\n",
    "\n",
    "model_name_list = []\n",
    "model_name_list += [f'uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_08_cv5_{cv_fold_id}_epoch_24_raw.parquet' for cv_fold_id in range(5)] \n",
    "model_name_list += [f'uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_08_cv5_{cv_fold_id}_epoch_25_raw.parquet' for cv_fold_id in range(5)] \n",
    "\n",
    "model_name_list += [f'uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_09_cv5_{cv_fold_id}_epoch_24_raw.parquet' for cv_fold_id in range(5)] \n",
    "model_name_list += [f'uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_09_cv5_{cv_fold_id}_epoch_25_raw.parquet' for cv_fold_id in range(5)] \n",
    "\n",
    "model_name_list += [f'uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_10_cv5_{cv_fold_id}_epoch_24_raw.parquet' for cv_fold_id in range(5)] \n",
    "model_name_list += [f'uhf08_pretrained_url_256_tf_4_4_adamw_0.0001_1e-05_b_32_v13_01_10_cv5_{cv_fold_id}_epoch_25_raw.parquet' for cv_fold_id in range(5)] \n",
    "\n",
    "prediction = get_raw_predictions(model_name_list)\n",
    "\n",
    "prediction_mean = prediction.groupby('user_id').agg([\n",
    "    pl.col('is_male').mean(),\n",
    "    pl.col('age_pred')\n",
    "]).with_columns([\n",
    "    pl.col('age_pred').apply(lambda x: np.mean(np.vstack(np.array(x)), axis=0).tolist())\n",
    "]).sort('user_id')\n",
    "\n",
    "age_pred = np.vstack(prediction_mean['age_pred'].to_numpy()).argmax(axis=-1) + 1\n",
    "prediction_mean = prediction_mean.with_columns(pl.lit(age_pred).alias('age'))\n",
    "save_prediction(prediction_mean, f'v13_01_10_cv5_3_seeds')\n",
    "prediction_mean"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "notebookId": "ace572b6-259b-4103-83cc-990a2382ffbc",
  "notebookPath": "3_transformer_model.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
